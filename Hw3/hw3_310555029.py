# -*- coding: utf-8 -*-
"""hw3_310555029.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QmJOeh22SXbnKBmzTo2pvwn6ujbbZQ69
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import os
from sklearn import svm
from sklearn.decomposition import PCA
from scipy.stats import multivariate_normal


# you can choose one of the following package for image reading/processing
import cv2
import PIL

"""# 1 Support Vector Machine (SVM)"""

x_data = pd.read_csv("/content/drive/MyDrive/ML/hw3/x_train.csv",header = None)/255
t_train = pd.read_csv("/content/drive/MyDrive/ML/hw3/t_train.csv",header = None)
x_data = np.array(x_data)
t_train = np.array(t_train).flatten()

print(x_data.shape)
print(t_train.shape)

"""## PCA"""

def PCA(X):
  mean = np.mean(X, axis=0)
  std = np.std(X, axis=0, ddof=0)
  X = X - mean
  cov = np.cov(X.T)
  eigenvalues, eigenvectors = np.linalg.eig(cov)
  eigenvectors = eigenvectors.T
  idxs = np.argsort(eigenvalues)[::-1]
  eigenvalues = eigenvalues[idxs] 
  eigenvectors = eigenvectors[idxs]
  components = eigenvectors[0:2]
  return np.real(np.dot(X, components.T))

x_train = PCA(x_data)
print(x_train.shape)

"""## SVM"""

class SVM:
  def __init__(self,x_train,t_train ,C=1, kernel_type='linear'):
    self.x = x_train
    self.y = t_train
    self.C = C
    self.class_order = [(0, 1), (0, 2), (1, 2)]
    self.kernel_type = kernel_type
    self.X, self.target = self.make_X_t(self.x,self.y) 
    self.weights, self.bias, self.support_vector = self.cal_w_b(self.X, self.target)
    self.xx, self.yy = self.make_meshgrid(x_train[:,0], x_train[:,1])

  def make_X_t(self,x,y):
    X={}
    target={}
    for i in self.class_order:
      target[i] = np.concatenate((np.ones(100),np.full([100], -1)))
      class1_index = np.where(y == i[0])
      class2_index = np.where(y == i[1])
      X[i] = np.vstack((x[class1_index],x[class2_index]))
    return X,target

    
  def cal_phix(self,X):
    if self.kernel_type == 'linear':
      return X
    else:
      temp = []
      for x in X:
        temp.append([x[0]**2, np.sqrt(2)*x[0]*x[1], x[1]**2])
      temp = np.array(temp)
      return temp
  
  def kernel_function(self, xi, xj):
    return np.dot(xi, xj.T)
  
  def target_for_oao(self,t_train):
    # class1={1}, class2={-1}, other class={0}
    target= {}
    for i in self.class_order:
      target[i] = np.zeros(self.x.shape[0])
      class1_index = np.where(t_train==i[0])
      class2_index = np.where(t_train==i[1])
      target[i][class1_index] = 1
      target[i][class2_index] = -1
    print(target)
    return target

  def cal_w_b(self,X,target):
    weights = {}
    bias = {}
    support_vector = {}
    for i in self.class_order:
      if self.kernel_type == 'linear':
        clf = svm.SVC(kernel='linear', decision_function_shape='ovo')
      else:
        clf = svm.SVC(kernel='poly', C=1.8 , degree=2, decision_function_shape='ovo')
      clf.fit(X[i],target[i])
      dual_coef = clf.dual_coef_ # Lagrange multipliers中support vector的係數
      lagrange_coef = np.abs(clf.dual_coef_)
      support_index = clf.support_ #在train dataset各類的support vector的index
      support_vector[i] = clf.support_vectors_
      phix = self.cal_phix(support_vector[i])
      #print(phix)
      t = target[i]
      y = []
      y.append(t[support_index])
      y = np.array(y)
      #公式(7.29)計算weights
      weights[i] = np.dot(dual_coef, phix)
      #公式(7.37)計算bias
      index_n = np.where((lagrange_coef[0] > 0) & (lagrange_coef[0] < self.C))
      index_m = np.nonzero(lagrange_coef)
      bias[i] = np.mean(y[0][index_n] - np.dot(phix[index_n], weights[i].T))
    return weights, bias, support_vector
  
  def make_meshgrid(self, x, y): 
    space = 0.01
    h=0.02
    x_min, x_max = x.min() - space, x.max() + space
    y_min, y_max = y.min() - space, y.max() + space
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))
    return xx, yy

  def predict(self,):
    X = np.c_[self.xx.flatten(), self.yy.flatten()]
    if self.kernel_type =='poly':
      X_poly=[]
      for x in X:
        X_poly.append([x[0]**2, np.sqrt(2)*x[0]*x[1], x[1]**2])
      X = np.array(X_poly)
    else:
      X=X

    y = {}
    for i in self.class_order:
      weights = self.weights[i]
      bias = self.bias[i]
      #x = self.cal_phix(self.X[i])
      y[i] =  np.sign(np.dot(weights, X.T) + bias)
    predict = []
    for i in range(X.shape[0]):
      predict_class = {0: 0, 1: 0, 2: 0}
      for c1,c2 in self.class_order:
        if y[(c1,c2)][0][i] == 1:
          predict_class[c1]+=1
        else:
          predict_class[c2]+=1
      predict.append(max(predict_class, key=predict_class.get))
    predict = np.array(predict)
    predict = predict.reshape(self.xx.shape)
    return predict

  def plot(self,predict):
    plt.title(self.kernel_type)
    plt.scatter(self.support_vector[(0,1)][:,0], self.support_vector[(0,1)][:,1], facecolors='none', edgecolors='k', linewidths=2, label="support vector")
    plt.scatter(self.support_vector[(0,2)][:,0], self.support_vector[(0,2)][:,1], facecolors='none', edgecolors='k', linewidths=2)
    plt.scatter(self.support_vector[(1,2)][:,0], self.support_vector[(1,2)][:,1], facecolors='none', edgecolors='k', linewidths=2)
    class1_index = np.where(self.y == 0)
    class2_index = np.where(self.y == 1)
    class3_index = np.where(self.y == 2)
    plt.scatter(x_train[class1_index][:, 0], x_train[class1_index][:, 1], c='b', marker='x', label="chinese 1")
    plt.scatter(x_train[class2_index][:, 0], x_train[class2_index][:, 1], c='r', marker='*', label="chinese 2")
    plt.scatter(x_train[class3_index][:, 0], x_train[class3_index][:, 1], c='g', marker='o', label="chinese 4")
    plt.contourf(self.xx, self.yy, predict, cmap=plt.cm.brg, alpha=0.3)
    plt.legend()
    plt.show()

"""### Linear SVM"""

linear_model = SVM(x_train, t_train) 
predict = linear_model.predict()
linear_model.plot(predict)

"""### Poly SVM"""

poly_model = SVM(x_train, t_train, kernel_type='poly') 
predict = poly_model.predict()
poly_model.plot(predict)

"""# 2 Gaussian Mixture Model"""

img = PIL.Image.open('/content/drive/MyDrive/ML/hw3/hw3.jpg')
img = np.asarray(img)/255
X = img.reshape(-1,3)
print(X.shape)

"""## K-means model"""

image = PIL.Image.open('/content/drive/MyDrive/ML/hw3/hw3.jpg')
img = np.asarray(image)/255
X = img.reshape(img.shape[0]*img.shape[1],3)
print(X.shape)

class Kmeans:
  def __init__(self, X, k):
    self.X = X
    self.k = k
    self.clusters = np.zeros((X.shape[0],1))
    self.centroids = X[np.random.choice(len(X), k)] # 隨機地取K個向量當成每群的中心點

  def make_cluster(self,centroids):
    clusters = np.zeros((self.X.shape[0],1))
    temp = np.empty((self.X.shape[0],1))
    for i in range(self.k):
      mu_k = np.ones((self.X.shape[0],1))*centroids[i]
      each_distance = np.sum((self.X-mu_k)**2,axis=1)
      each_distance = np.array(each_distance)
      each_distance.resize((self.X.shape[0],1))
      temp = np.append(temp,each_distance,axis=1)
    temp = np.delete(temp,0,axis=1)
    clusters = np.argmin(temp, axis=1) # 每個向量被分類到距離最短的中心
    return clusters
  
  def update_centroids(self,clusters):
    new_centroids = np.empty((self.k, self.X.shape[1]))
    for i in range(self.k):
      #對每一群算出新的向量平均值，以此做為新的群中心
      new_centroids[i] = np.mean(self.X[np.where(clusters==i)],axis=0)
    new_centroids = np.array(new_centroids)
    return new_centroids

  def predict(self):
    centroids = self.centroids
    while True:
      # 更新 cluster
      self.clusters = self.make_cluster(self.centroids)
      # 更新 centroid
      centroids = self.update_centroids(self.clusters)
      #將新的群中心與舊的群中心作比較，如果不再有變動，表示已收斂
      if(centroids == self.centroids).all():
        break
      #else:
      self.centroids = centroids
    return centroids, self.clusters

  def show_img(self,centroids, cluster_labels):
    print("Kmeans ",self.k)
    for i in range(self.k):
      print(str(i+1), centroids[i])
    idx = self.make_cluster(centroids)
    image = np.reshape(centroids[idx,:],(img.shape[0],img.shape[1],3))
    plt.figure()
    plt.imshow(image)
    plt.show()

K = [3, 7, 10, 30]
centroids_sets = []
cluster_labels_sets = []
for k in K:
  model = Kmeans(X, k=k)
  centroids, cluster_labels= model.predict()
  centroids_sets.append(centroids)
  cluster_labels_sets.append(cluster_labels)
  model.show_img(centroids, cluster_labels)

"""# Gaussian Mixture Model"""

class GMM:
  def __init__(self,X , k=3, max_iter=100):
    self.X = X
    self.k = k
    self.max_iter = max_iter
    self.loglikelihood = []
    self.pi = None
    self.mu = None
    self.cov = None

  def cal_pi_mu_cov(self,centroids, predict_y):
    pi = [np.mean(predict_y == k) for k in range(self.k)] 
    pi = np.array(pi)
    self.pi = pi
    mu = centroids 
    mu = np.array(mu)
    self.mu = mu
    cov = [np.cov(X[np.where(predict_y==k)[0]].T) for k in range(self.k)]
    con = np.array(cov)
    self.cov = cov
    return pi,mu,cov  

  def E_step(self, pi, mu, cov):
    #multivariate_normal.pdf(self.X, mu[k], cov[k])
    likelihood = np.array([pi[k]*multivariate_normal.pdf(self.X, mean=mu[k], cov=cov[k], allow_singular=True) for k in range(self.k)]) 
    gamma = np.array(likelihood / np.sum(likelihood, axis=0)).T 
    return gamma, likelihood

  def M_step(self,gamma):
    N_k = np.sum(gamma, axis=0) 
    pi = N_k / self.X.shape[0] 
    mu = np.dot(gamma.T, self.X) /  N_k[:, np.newaxis] 
    cov = [(1/N_k[k]) * np.dot((gamma[:, k, np.newaxis]*(self.X - mu[k])).T, (self.X - mu[k])) for k in range(self.k)]
    return pi, mu, cov

  def EM_algorithm(self):
    for i in range(self.max_iter):
      # E step
      self.gamma, self.likelihood = self.E_step(self.pi, self.mu, self.cov)
      # M step
      self.pi, self.mu, self.cov = self.M_step(self.gamma)
      # log likelihood
      loglikelihood = np.sum(np.log(np.sum(self.likelihood, axis=0)))
      self.loglikelihood.append(loglikelihood)
    return self.likelihood, self.mu
    
  def show_GMM_img(self, likelihood, mu):
    idx = np.argmax(likelihood, axis=0)
    image = (mu[idx]*255).astype(int)
    image = PIL.Image.fromarray(image.reshape(img.shape[0], img.shape[1], 3).astype('uint8'))
    plt.title(f"GMM for k = {self.k}")
    plt.imshow(image)
    plt.show()
  def show_loglikelihood(self):
    plt.title('Log likelihood for k='+str(self.k))
    plt.xlabel('iteration')
    plt.ylabel('Log likelihood value')
    plt.plot(self.loglikelihood, label = 'log_likelihood')
    plt.legend()
    plt.show()

K = [3, 7, 10, 30]
for idx, k in enumerate(K):
  #model_Kmeans = Kmeans(X, k=k)
  #centroids, y_pred = model_Kmeans.predict()
  centroids = centroids_sets[idx]
  y_pred = cluster_labels_sets[idx]
                
  model_GMM = GMM(X,k=k,max_iter=100)
  pi, mu, cov = model_GMM.cal_pi_mu_cov(centroids,y_pred)
  likelihood, mu = model_GMM.EM_algorithm()

  model_GMM.show_GMM_img(likelihood, mu)
  model_GMM.show_loglikelihood()